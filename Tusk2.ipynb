{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a031b3-461a-4609-b27f-2dd396a7c4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreyvasilyev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andreyvasilyev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk. tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer \n",
    "nltk.download ('punkt')\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction. text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve \n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f36978-9328-4eb9-a041-80c4482c2544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab3c257-5fd3-491c-9028-36505c407aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.read_csv('train-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "123e4036-7625-44b3-a9ac-5d1147dfd36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And this year, the number will be over 150,000...</td>\n",
       "      <td>{'science', 'family', 'mobility'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MR. SPICER: I think the campaign will make dec...</td>\n",
       "      <td>{'economy', 'languages', 'style'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You dont have to test every person in the stat...</td>\n",
       "      <td>{'family'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And Dr. Fauci is going to emphasize this about...</td>\n",
       "      <td>{'science', 'family', 'history'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SANDERS: Certainly in a number of the conversa...</td>\n",
       "      <td>{'science', 'news'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14919973</th>\n",
       "      <td>Another publisher,Ananth Padmanabhan,VP,Sales...</td>\n",
       "      <td>{'mobility', 'sports', 'style'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14919974</th>\n",
       "      <td>Priya81794772 U r the winner DeshKiAwaazShehn...</td>\n",
       "      <td>{'business', 'health', 'affair'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14919975</th>\n",
       "      <td>Once we do that,we will be in a better positi...</td>\n",
       "      <td>{'mobility', 'politics', 'affair'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14919976</th>\n",
       "      <td>okokayae 200221 ShowcasennAllyxGraynALLYnHowT...</td>\n",
       "      <td>{'science', 'health', 'affair'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14919977</th>\n",
       "      <td>Tejpal reached Goa at 5</td>\n",
       "      <td>{'science', 'languages', 'politics'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14919978 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text  \\\n",
       "0         And this year, the number will be over 150,000...   \n",
       "1         MR. SPICER: I think the campaign will make dec...   \n",
       "2         You dont have to test every person in the stat...   \n",
       "3         And Dr. Fauci is going to emphasize this about...   \n",
       "4         SANDERS: Certainly in a number of the conversa...   \n",
       "...                                                     ...   \n",
       "14919973   Another publisher,Ananth Padmanabhan,VP,Sales...   \n",
       "14919974   Priya81794772 U r the winner DeshKiAwaazShehn...   \n",
       "14919975   Once we do that,we will be in a better positi...   \n",
       "14919976   okokayae 200221 ShowcasennAllyxGraynALLYnHowT...   \n",
       "14919977                            Tejpal reached Goa at 5   \n",
       "\n",
       "                                        labels  \n",
       "0            {'science', 'family', 'mobility'}  \n",
       "1            {'economy', 'languages', 'style'}  \n",
       "2                                   {'family'}  \n",
       "3             {'science', 'family', 'history'}  \n",
       "4                          {'science', 'news'}  \n",
       "...                                        ...  \n",
       "14919973       {'mobility', 'sports', 'style'}  \n",
       "14919974      {'business', 'health', 'affair'}  \n",
       "14919975    {'mobility', 'politics', 'affair'}  \n",
       "14919976       {'science', 'health', 'affair'}  \n",
       "14919977  {'science', 'languages', 'politics'}  \n",
       "\n",
       "[14919978 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "757e39ac-2b17-4060-bf38-694ae031e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, df=train_test_split(test_df,test_size=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7c94f9-afbc-4e72-92be-0001cc48b8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aefbd731-42b9-4992-bf8d-c01c6bf7785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_example = df. iloc [1] [\"text\" ]\n",
    "\n",
    "tokens = word_tokenize(sentence_example, language=\"english\")\n",
    "\n",
    "tokens_without_punctuation = [i for i in tokens if i not in string.punctuation]\n",
    "\n",
    "english_stop_words = stopwords.words (\"english\")\n",
    "\n",
    "tokens_without_stop_words_and_punctuation = [i for i in tokens_without_punctuation if i not in english_stop_words]\n",
    "\n",
    "snowball = SnowballStemmer (language=\"english\")\n",
    "\n",
    "stemmed_tokens = [snowball.stem(i) for i in tokens_without_stop_words_and_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa32823-90de-4845-a7ef-f619bcd69661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: I had a teacher in highschool that broke it into 10% bins and made it my goal to get 1 in each bin, cept 2 for the 100% bin.\n",
      "———————-\n",
      "Токены: ['I', 'had', 'a', 'teacher', 'in', 'highschool', 'that', 'broke', 'it', 'into', '10', '%', 'bins', 'and', 'made', 'it', 'my', 'goal', 'to', 'get', '1', 'in', 'each', 'bin', ',', 'cept', '2', 'for', 'the', '100', '%', 'bin', '.']\n",
      "————————-\n",
      "Токены без пунктуации: ['I', 'had', 'a', 'teacher', 'in', 'highschool', 'that', 'broke', 'it', 'into', '10', 'bins', 'and', 'made', 'it', 'my', 'goal', 'to', 'get', '1', 'in', 'each', 'bin', 'cept', '2', 'for', 'the', '100', 'bin']\n",
      "—————————\n",
      "Токены без пунктуации и стоп слов: ['I', 'teacher', 'highschool', 'broke', '10', 'bins', 'made', 'goal', 'get', '1', 'bin', 'cept', '2', '100', 'bin']\n",
      "—————\n",
      "Токены после стемминга: ['i', 'teacher', 'highschool', 'broke', '10', 'bin', 'made', 'goal', 'get', '1', 'bin', 'cept', '2', '100', 'bin']\n",
      "————\n"
     ]
    }
   ],
   "source": [
    "print(f\"Исходный текст: {sentence_example}\")\n",
    "print(\"———————-\")\n",
    "print (f\"Токены: {tokens}\")\n",
    "print(\"————————-\")\n",
    "print (f\"Токены без пунктуации: {tokens_without_punctuation}\")\n",
    "print(\"—————————\")\n",
    "print(f\"Токены без пунктуации и стоп слов: {tokens_without_stop_words_and_punctuation}\")\n",
    "print (\"—————\")\n",
    "print (f\"Токены после стемминга: {stemmed_tokens}\")\n",
    "print(\"————\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e487de-a91a-4af6-8c23-0158991bb759",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer (language=\"english\" )\n",
    "english_stop_words = stopwords.words(\"english\")\n",
    "def tokenize_sentence(sentence: str, remove_stop_words: bool = True):\n",
    "    tokens = word_tokenize(sentence, language=\"english\" )\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    if remove_stop_words:\n",
    "        tokens = [i for i in tokens if i not in english_stop_words]\n",
    "    tokens = [snowball.stem(i) for i in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cac239-6629-4707-b2f7-e88a3d4cfd01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d71841d-d8f6-482b-b6f9-10d93a41a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenize_sentence(x, remove_stop_words=True),token_pattern=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f21610a-6bf3-4378-bd38-3ff0545c8a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(df[\"text\"].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f9bab-5134-4054-9061-ff596edf06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression (random_state=0,max_iter=15000000)\n",
    "model.fit(features, df[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b696ce-0a60-4703-a7e4-992f94576a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
